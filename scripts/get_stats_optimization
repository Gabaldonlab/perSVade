#!/usr/bin/env python

######################################
############# DEFINE ENV #############
######################################

# general module imports
import argparse, os
from argparse import RawTextHelpFormatter
import copy as cp
import pandas as pd
import pickle
import string
import shutil 
import random
import sys
from shutil import copyfile
import time

# get the cwd were all the scripts are 
CWD = "/".join(__file__.split("/")[0:-1]); sys.path.insert(0, CWD)

# define the module name
module_name = __file__.split("/")[-1]

# define the EnvDir where the environment is defined
EnvDir = "/".join(sys.executable.split("/")[0:-2])

# import functions
import sv_functions as fun

# import perSVade-specific modules

######################################
######################################
######################################


#################################### 
############## ARGS ################
####################################

description = """
Takes a tab-sepparated csv-file with 'sampleID' and 'sorted_bam' columns. The 'sorted_bam' column should have the paths to the bam files to analyze. It generates a table with the median coverage, read length and insert size for each sample, which are statistics that can help you understand how diverse are your samples in terms of parameter optimization. In addition, it proposes a minimal number of samples in which to run parameter optimization that recapitulate most of the sequencing data. This modules generates the table 'samples_parameter_optimization.tab', which includes the mapping between each sample and the proposed sample in which to run parameter optimization.
"""

# mandatory args
parser = argparse.ArgumentParser(description=description, formatter_class=RawTextHelpFormatter)
parser.add_argument("-o", "--outdir", dest="outdir", action="store", required=True, help="Output directory.")
parser.add_argument("--samples_file", dest="samples_file", action="store", required=True, help="Tab-sepparated csv-file with 'sampleID' and 'sorted_bam' columns. The 'sorted_bam' column should have the paths to the bam files to analyze.")
parser.add_argument("-r", "--ref", dest="ref", required=True, help="Reference genome. It has to end with .fasta.")
parser.add_argument("-mchr", "--mitochondrial_chromosome", dest="mitochondrial_chromosome", required=True, type=str, help="The name of the mitochondrial chromosome. If there is no mitochondria just put 'no_mitochondria'. If there is more than one mitochindrial scaffold, provide them as comma-sepparated IDs, like '--mitochondrial_chromosome chr_mito_1,chr_mito_2'.")

parser.add_argument("--overlap_coverage", dest="overlap_coverage", default=10, type=int, help="The maximum difference in coverage, in %, to say that two samples have the same coverage.")
parser.add_argument("--overlap_insert_size", dest="overlap_insert_size", default=10, type=int, help="The maximum difference in insert size, in %, to say that two samples have the same insert size.")
parser.add_argument("--overlap_insert_size_sd", dest="overlap_insert_size_sd", default=10, type=int, help="The maximum difference in insert size, in %, SD to say that two samples have the same insert size SD.")
parser.add_argument("--overlap_read_len", dest="overlap_read_len", default=10, type=int, help="The maximum difference in read length, in %, to say that two samples have the same read length.")

# optional args
parser.add_argument("--replace", dest="replace", action="store_true", help="Re-run all the steps by deleting the output directory.")
parser.add_argument("--rerun_sample_clustering", dest="rerun_sample_clustering", action="store_true", help="Re-run the clustering steps")
parser.add_argument("--skip_cleaning_tmpdir", dest="skip_cleaning_tmpdir", action="store_true", help="Skip cleaning the tmpdir.")
parser.add_argument("--verbose", dest="verbose", action="store_true", default=False, help="Print a verbose log.")
parser.add_argument("--max_median_insert_size", dest="max_median_insert_size", default=10000, type=int, help="This module calculates the median insert size of the input sorted bam. This value is expected to be low (i.e. < 1000). If it is very high it suggests that the input .bam is somehow corrupt. This argument allows you to control what is considered as a very high insert size (default 10,000 bp). For example, if you want to allow a higher threshold you may specify '--max_median_insert_size 20000'.")
parser.add_argument("--min_chromosome_len", dest="min_chromosome_len", default=100000, type=int, help="The minimum length to consider chromosomes from the provided fasta for calculating the window length (used in may steps of perSVade to parallelize across fractions of the genome).")

# resources
parser.add_argument("--min_Gb_per_core", dest="min_Gb_per_core", default=4, type=int, help="Minimum number of Gb per core to be used in the parallelized calculations of coverage, insert size and read length.")
parser.add_argument("--fraction_available_mem", dest="fraction_available_mem", default=None, type=float, help="This pipeline calculates the available RAM for several steps, and it may not work well in some systems (i.e. HPC clusters). This parameter allows you to correct possible errors. If --fraction_available_mem is not provided (default behavior), this pipeline will calculate the available RAM by filling the memory, which may give errors. If you want to use all the available memory you should specify --fraction_available_mem 1.0. See the FAQ 'How does the --fraction_available_mem work?' from https://github.com/Gabaldonlab/perSVade/wiki/8.-FAQs for more info.")

parser.add_argument("-thr", "--threads", dest="threads", default=16, type=int, help="Number of threads, Default: 16")
parser.add_argument("--fractionRAM_to_dedicate", dest="fractionRAM_to_dedicate", type=float,  default=0.5, help="This is the fraction of the available memory that will be used by several java programs that require a heap size. By default we set this to 0.5 to not overload the system.")

opt = parser.parse_args()

####################################
####################################
####################################

#################################
########### MAIN CODE ###########
#################################

# remove outdir if replace, and set replace to False
if opt.replace is True: fun.delete_folder(opt.outdir)
opt.replace = False

# make the outdir
fun.make_folder(opt.outdir)

# exit if the final file exists
final_file = "%s/perSVade_finished_file.txt"%opt.outdir

# run again 
if opt.rerun_sample_clustering is True: fun.remove_file(final_file)

# skip if done
if not fun.file_is_empty(final_file): 
    fun.print_with_runtime("WARNING: %s exists, suggesting that perSVade was already  run in this folder. Remove this file if you want this command to work. Exiting..."%final_file)
    sys.exit(0)

# define the start time
start_time = time.time()

# define the verbosity. If opt.verbose is False, none of the 'print' statements of sv_functions will have an effect
fun.printing_verbose_mode = opt.verbose

# define a file that will contain all the cmds ran
fun.log_file_all_cmds = "%s/all_cmds.txt"%opt.outdir
if fun.file_is_empty(fun.log_file_all_cmds): open(fun.log_file_all_cmds, "w").write("# These are all the cmds:\n")

# get sample name
sample_name = fun.get_sampleName_from_perSVade_outdir(opt.outdir)

####### SET RESOURCES ########

# define the fraction of RAM to dedicate
if opt.fractionRAM_to_dedicate>0.95: raise ValueError("You are using >95 pct of the systems RAM, which is dangerous")
fun.fractionRAM_to_dedicate = opt.fractionRAM_to_dedicate

# define the fraction of available mem
fun.fraction_available_mem = opt.fraction_available_mem
if opt.fraction_available_mem is None: fun.print_with_runtime("WARNING: You did not specify how much RAM should be used through --fraction_available_mem. perSVade will calculate this by filling the memory, which may be dangerous. If you want to use all the allocated memory you should specify --fraction_available_mem 1.0")

# print the available resources
real_available_threads = fun.get_available_threads(opt.outdir)
if opt.threads>real_available_threads:  fun.print_with_runtime("WARNING: There are %i available threads, and you required %i."%(real_available_threads, opt.threads))

available_Gb_RAM = fun.get_availableGbRAM(opt.outdir)
fun.print_with_runtime("Running perSVade %s into %s with %.3f Gb of RAM and %i cores. Two samples are considered to be equivalent if their coverages are <%i different, the insert size is <%i appart, the insert size SD is <%i different and the read length differs by <%i. These numbers are percentages. "%(module_name, opt.outdir, available_Gb_RAM, opt.threads, opt.overlap_coverage, opt.overlap_insert_size, opt.overlap_insert_size_sd, opt.overlap_read_len))

##############################

# prepare the reference genome
opt.ref, reference_genome_dir = fun.prepare_reference_genome_for_perSVade(opt.ref, opt.outdir, opt.mitochondrial_chromosome, None, opt.replace)
fun.window_l = fun.get_perSVade_window_l(opt.ref, opt.mitochondrial_chromosome, opt.min_chromosome_len)

# clean reference
fun.clean_reference_genome_windows_files(opt.ref)

# define the tmpdir
tmpdir_bams = "%s/tmp_bams"%opt.outdir; fun.make_folder(tmpdir_bams)

# load df with the samples
df_samples = fun.get_tab_as_df_or_empty_df(opt.samples_file)
sorted_samples = sorted(set(df_samples.sampleID))
if len(sorted_samples)!=len(df_samples): raise ValueError("sampleIDs should be unique")

# calculate stats for each sample
max_ncores_parallel = int(available_Gb_RAM/opt.min_Gb_per_core)
ncores_parallel = min([opt.threads, max_ncores_parallel])

fun.print_with_runtime("Getting sequencing parameters in parallel on %i cores (ensuring >%iGb per core)"%(ncores_parallel, opt.min_Gb_per_core))
inputs_fn = [(sampleID, input_sorted_bam, tmpdir_bams, opt.replace, 1, opt.max_median_insert_size, opt.ref, opt.mitochondrial_chromosome) for sampleID, input_sorted_bam in df_samples[["sampleID", "sorted_bam"]].values]

with fun.multiproc.Pool(ncores_parallel) as pool:
    df_parameters = pd.DataFrame(pool.starmap(fun.get_sequencing_parameters_dict_one_sampleID, inputs_fn)).set_index("sampleID", drop=False)
    pool.close()
    pool.terminate()

# sort df in a way that puts worst samples first
df_parameters = df_parameters.sort_values(by=["gDNA_median_coverage", "mtDNA_median_coverage", "read_length", "median_insert_size", "median_insert_size_sd"], ascending=[True, True, True, True, False])

# define the pairs of similar samples according to the overlapping distances parameters
fun.print_with_runtime("Calculating pairs of strains that have similar parameters")
field_to_overlapping_distance = {"gDNA_median_coverage":opt.overlap_coverage, "mtDNA_median_coverage":opt.overlap_coverage, "read_length":opt.overlap_read_len, "median_insert_size":opt.overlap_insert_size, "median_insert_size_sd":opt.overlap_insert_size_sd,}

pairs_similar_samples = set()
for s1 in sorted_samples:
    for s2 in sorted_samples:
        if s1==s2: continue
        s1_r = df_parameters.loc[s1]
        s2_r = df_parameters.loc[s2]

        if fun.get_samples_are_similar_by_seq_parameters(s1_r, s2_r, field_to_overlapping_distance): pairs_similar_samples.add((s1, s2))

# check that the pairs are in both directions
for s1, s2 in pairs_similar_samples:
    if (s2, s1) not in pairs_similar_samples: raise ValueError("%s,%s should be in pairs_similar_samples"%(s2, s1))

# define clusters of similar samples. Iterate starting with the worst samples
repSample_to_similarSamples = {}

for I,r in df_parameters.iterrows():

    # if this sample is similar to previous rep samples, add it in the cluster
    similar_repSample_found = False
    for repSample, similarSamples in repSample_to_similarSamples.items():

        if (r.sampleID, repSample) in pairs_similar_samples: 
            repSample_to_similarSamples[repSample].add(r.sampleID)
            similar_repSample_found = True
            break

    # if not similar samples were found, init a new repSample
    if similar_repSample_found is False: repSample_to_similarSamples[r.sampleID] = {r.sampleID}

# map each sample to the rep sample
sampleID_to_repSample = {}
for s in sorted_samples:
    rep_samples = [repS for repS, sim_s in repSample_to_similarSamples.items() if s in sim_s]
    if len(rep_samples)!=1: raise ValueError("sample %s is in the following rep_samples: %s"%(s, rep_samples))
    sampleID_to_repSample[s] = rep_samples[0]


# create final file with all data
df_parameters["representative_sampleID"] = df_parameters.sampleID.apply(lambda x: sampleID_to_repSample[x])
fun.save_df_as_tab(df_parameters[["sampleID", "representative_sampleID", "gDNA_median_coverage", "mtDNA_median_coverage", "read_length", "median_insert_size", "median_insert_size_sd"]], "%s/samples_parameter_optimization.tab"%opt.outdir)
fun.print_with_runtime("There are %i/%i representative samples"%(len(set(df_parameters.representative_sampleID)), len(sorted_samples)))

#################################
#################################
#################################


##################################
########## CLEAN OUTPUT ##########
##################################

# clean
if opt.skip_cleaning_tmpdir is False: fun.delete_folder(tmpdir_bams)
fun.delete_file_or_folder(reference_genome_dir)

# wite final file
fun.generate_final_file_report_one_module(final_file, start_time, time.time())

# print the message
fun.print_with_runtime("perSVade %s finished correctly"%module_name)

##################################
##################################
##################################
