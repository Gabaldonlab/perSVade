#!/usr/bin/env python

######################################
############# DEFINE ENV #############
######################################

# general module imports
import argparse, os
from argparse import RawTextHelpFormatter
import copy as cp
import pickle
import string
import shutil 
import random
import sys
from shutil import copyfile
import time

# get the cwd were all the scripts are 
CWD = "/".join(__file__.split("/")[0:-1]); sys.path.insert(0, CWD)

# define the module name
module_name = __file__.split("/")[-1]

# define the EnvDir where the environment is defined
EnvDir = "/".join(sys.executable.split("/")[0:-2])

# import functions
import sv_functions as fun

# import perSVade-specific modules
import pandas as pd

# define binaries
samtools = "%s/bin/samtools"%EnvDir
java = "%s/bin/java"%EnvDir
bcftools = "%s/bin/bcftools"%EnvDir
bedtools = "%s/bin/bedtools"%EnvDir

######################################
######################################
######################################


#################################### 
############## ARGS ################
####################################

description = """
This module runs in two steps:

- 1) Variant calling: It runs either GATK HaplotypeCaller, bcftools call and/or freebayes from an input sorted bam to find SNPs and IN/DELs. It filters the variants according to the best practices and a user provided --min_coverage argument.

- 2) Variant integration: It integrates the variants into different types of merged files, which is useful to compute consensus sets of variants.

"""

parser = argparse.ArgumentParser(description=description, formatter_class=RawTextHelpFormatter)

# mandatory args
parser.add_argument("-o", "--outdir", dest="outdir", action="store", required=True, help="Output directory.")
parser.add_argument("-r", "--ref", dest="ref", required=True, help="Reference genome. It has to end with .fasta.")
parser.add_argument("-sbam", "--sortedbam", dest="sortedbam", required=True, help="The path to the sorted .bam file, which should have a bam.bai file in the same dir. For example, if your bam file is called 'aligned_reads.bam', there should be an 'aligned_reads.bam.bai' as well.")
parser.add_argument("--repeats_file", dest="repeats_file", required=True, help="A file with the repeats of the reference genome, such as the file 'combined_repeats.tab' generated by perSVade infer_repeats. You may set '--repeats_file skip' if you don't want to consider repeats.")
parser.add_argument("-p", "--ploidy", dest="ploidy", required=True, type=int, help="Ploidy, can be 1 or 2.")
parser.add_argument("--callers", dest="callers", required=True, help="Variant callers in a comma sepparated manner. If you want to run all of them (default) specify '--callers HaplotypeCaller,bcftools,freebayes'.")
parser.add_argument("--min_AF", dest="min_AF", required=True, type=float, help="The minimum fraction of reads covering a variant to be preserved after filtering. By default, we recommend setting this to 0.25 (i.e. '--min_AF 0.25') for diploids and 0.9 for haploids. It should be a float between 0 and 1. Note that this filter only has effect on the final, merged .vcf files (called 'variants_atLeast<n_PASS_programs>PASS_ploidy<ploidy>.vcf')")
parser.add_argument("-c", "--min_coverage", dest="min_coverage", required=True, type=int, help="The minimum coverage (int) of a locus to be considered for variant calling. In HaplotypeCaller and bcftools, positions with a lower coverage will be filtered out in the filtered vcfs. In freebayes, positions with a lower coverage will not even be considered for variant calling. You should take into account the coverage of your sample to set this, which can be calculated with 'perSVade align_reads' across windows of the genome. For example, if your average coverage is 100x, you may set '--min_coverage 20'. Note that the meaning of this argument is different if you specify --pooled_sequencing.")

# optional args
parser.add_argument("--pooled_sequencing", dest="pooled_sequencing", action="store_true", default=False, help="It runs variant calling assuming that the input is a pool of different strains (i.e. a population). If provided, the only caller used will be 'freebayes'. In addition, note that the only positions considered for variant calling will be those with at least one allele with more reads than --min_coverage.")
parser.add_argument("--stop_before_VariantIntegration", dest="stop_before_VariantIntegration", action="store_true", default=False, help="It stops the execution after the 'Variant calling' step.")
parser.add_argument("--window_freebayes_bp", dest="window_freebayes_bp", default=10000, type=int, help="The window (in bp) in which freebayes regions are split to run in parallel. If you increase this number the splitting will be in larger chunks of the genome.")
parser.add_argument("--fb_use_best_n_alleles", dest="fb_use_best_n_alleles", default=20, type=int, help="The maximum number of alleles considered by freebayes in pooled mode. Default is 20.")
parser.add_argument("--min_chromosome_len", dest="min_chromosome_len", default=100000, type=int, help="The minimum length to consider chromosomes from the provided fasta for calculating the window length (used in may steps of perSVade to parallelize across fractions of the genome).")
parser.add_argument("--outdir_callCNVs", dest="outdir_callCNVs", default=None, help="The path to the output directory of the 'call_CNVs' module. If provided, this module will add a 'relative_CN' column to the file 'variant_calling_ploidy<ploidy>.tab', which includes the relative copy number of the region in which each variant is found. This may be used to filter out variants that are in certain regions (i.e. duplicated or deleted).")
parser.add_argument("--replace", dest="replace", action="store_true", help="Re-run all the steps by deleting the output directory.")
parser.add_argument("--verbose", dest="verbose", action="store_true", default=False, help="Print a verbose log.")

# resources
parser.add_argument("--fraction_available_mem", dest="fraction_available_mem", default=None, type=float, help="This pipeline calculates the available RAM for several steps, and it may not work well in some systems (i.e. HPC clusters). This parameter allows you to correct possible errors. If --fraction_available_mem is not provided (default behavior), this pipeline will calculate the available RAM by filling the memory, which may give errors. If you want to use all the available memory you should specify --fraction_available_mem 1.0. See the FAQ 'How does the --fraction_available_mem work?' from https://github.com/Gabaldonlab/perSVade/wiki/8.-FAQs for more info.")

parser.add_argument("-thr", "--threads", dest="threads", default=16, type=int, help="Number of threads, Default: 16")
parser.add_argument("--fractionRAM_to_dedicate", dest="fractionRAM_to_dedicate", type=float,  default=0.5, help="This is the fraction of the available memory that will be used by several java programs that require a heap size. By default we set this to 0.5 to not overload the system.")

opt = parser.parse_args()


####################################
####################################
####################################

#################################
########### MAIN CODE ###########
#################################

# remove outdir if replace, and set replace to False
if opt.replace is True: fun.delete_folder(opt.outdir)
opt.replace = False

# make the outdir
fun.make_folder(opt.outdir)

# exit if the final file exists
final_file = "%s/perSVade_finished_file.txt"%opt.outdir

if not fun.file_is_empty(final_file): 
    fun.print_with_runtime("WARNING: %s exists, suggesting that perSVade was already  run in this folder. Remove this file if you want this command to work. Exiting..."%final_file)
    sys.exit(0)

# define the start time
start_time = time.time()

# define the verbosity. If opt.verbose is False, none of the 'print' statements of sv_functions will have an effect
fun.printing_verbose_mode = opt.verbose

# define a file that will contain all the cmds ran
fun.log_file_all_cmds = "%s/all_cmds.txt"%opt.outdir
if fun.file_is_empty(fun.log_file_all_cmds): open(fun.log_file_all_cmds, "w").write("# These are all the cmds:\n")

# get sample name
sample_name = fun.get_sampleName_from_perSVade_outdir(opt.outdir)

####### SET RESOURCES ########

# define the fraction of RAM to dedicate
if opt.fractionRAM_to_dedicate>0.95: raise ValueError("You are using >95 pct of the systems RAM, which is dangerous")
fun.fractionRAM_to_dedicate = opt.fractionRAM_to_dedicate

# define the fraction of available mem
fun.fraction_available_mem = opt.fraction_available_mem
if opt.fraction_available_mem is None: fun.print_with_runtime("WARNING: You did not specify how much RAM should be used through --fraction_available_mem. perSVade will calculate this by filling the memory, which may be dangerous. If you want to use all the allocated memory you should specify --fraction_available_mem 1.0")

# print the available resources
real_available_threads = fun.get_available_threads(opt.outdir)
if opt.threads>real_available_threads:  fun.print_with_runtime("WARNING: There are %i available threads, and you required %i."%(real_available_threads, opt.threads))

available_Gb_RAM = fun.get_availableGbRAM(opt.outdir)
fun.print_with_runtime("Running perSVade %s into %s with %.3f Gb of RAM and %i cores"%(module_name, opt.outdir, available_Gb_RAM, opt.threads))

##############################

# prepare the reference genome
opt.ref, reference_genome_dir = fun.prepare_reference_genome_for_perSVade(opt.ref, opt.outdir, "no_mitochondria", None, opt.replace)
fun.window_l = fun.get_perSVade_window_l(opt.ref, "no_mitochondria", opt.min_chromosome_len)

# get the bam file under output
sorted_bam, index_bam = fun.get_sorted_bam_in_outdir(opt.sortedbam, opt.outdir)

# prepare the repeats file
fun.prepare_repeats_file_for_perSVade(opt.repeats_file, opt.ref)
repeats_table = opt.ref+".repeats.tab"

# define and debug the callers
callers = set(opt.callers.split(","))
all_expected_callers = {"freebayes", "HaplotypeCaller", "bcftools"}
strange_callers = callers.difference(all_expected_callers)
if len(strange_callers)>0 or len(callers)==0: raise ValueError("the --callers argument is empty or wrong. These are the strange callers: %s"%strange_callers)

# check the min_AF
if opt.min_AF<=1 and opt.min_AF>=0: min_AF = float(opt.min_AF)
else: raise ValueError("The value provided in --min_AF is incorrect")

# if pooled seq is specified, just use freebayes
if opt.pooled_sequencing is True: callers = {"freebayes"}

#### VARIANT CALLING #######

# log
fun.print_with_runtime("Running small variant calling.")

# initialize an array of files that have the VCF results filtered
filtered_vcf_results = []

if "HaplotypeCaller" in callers:
    fun.print_with_runtime("Running GATK HaplotypeCaller")

    # create a folder that will contain the output of VCF
    outdir_gatk = "%s/HaplotypeCaller_ploidy%i_out"%(opt.outdir, opt.ploidy)

    # run gatk and get the filtered filename
    gatk_out_filtered = fun.run_gatk_HaplotypeCaller(outdir_gatk, opt.ref, sorted_bam, opt.ploidy, opt.threads, opt.min_coverage, replace=opt.replace)

    # keep
    filtered_vcf_results.append(gatk_out_filtered)
   

if "bcftools" in callers:
    fun.print_with_runtime("Running bcftools call")

    # create a folder that will contain the output of VCF
    outdir_bcftools = "%s/bcftools_ploidy%i_out"%(opt.outdir, opt.ploidy)
    if not os.path.isdir(outdir_bcftools): os.mkdir(outdir_bcftools)

    # only continue if the final file is not done
    filtered_output = "%s/output.filt.vcf"%outdir_bcftools;     
    if fun.file_is_empty(filtered_output) or opt.replace is True:

        # look for the mpileup bcf in sister directories, as it is the same for any other ploidy
        mpileup_output = "%s/output.mpileup.bcf"%outdir_bcftools; mpileup_output_tmp = "%s.tmp"%mpileup_output
        for folder in os.listdir(opt.outdir):
            if folder.startswith("bcftools_ploidy") and folder.endswith("_out"):

                # look for the potential previously calculated mpielup outputs
                potential_previosuly_calculated_mpileup_output = "%s/%s/output.mpileup.bcf"%(opt.outdir, folder)
                if not fun.file_is_empty(potential_previosuly_calculated_mpileup_output): 
                    print("taking %s from previous run"%potential_previosuly_calculated_mpileup_output)
                    mpileup_output = potential_previosuly_calculated_mpileup_output; break

        # if there is no previous run
        if fun.file_is_empty(mpileup_output) or opt.replace is True:

            print("Running mpileup...")
            cmd_bcftools_mpileup = '%s mpileup -a "AD,DP" -O b -f %s -o %s --threads %i %s'%(bcftools, opt.ref, mpileup_output_tmp, opt.threads, sorted_bam); fun.run_cmd(cmd_bcftools_mpileup)
            os.rename(mpileup_output_tmp, mpileup_output)


        # run bcftools call
        call_output = "%s/output.raw.vcf"%outdir_bcftools; call_output_tmp = "%s.tmp"%call_output
        if fun.file_is_empty(call_output) or opt.replace is True:
            print("Running bcftools call ...")

            # define the ploidy specification
            if opt.ploidy==1: ploidy_cmd = "--ploidy %i"%opt.ploidy # This is all haploid
            else:
                # create a ploidy file if ploidy is 2. There's no way to simpli specify ploidy 2
                ploidy_file_bcftools = "%s/ploidy_file.tab"%outdir_bcftools
                open(ploidy_file_bcftools, "w").write("* * * * %i\n"%opt.ploidy) # CHROM, FROM, TO, SEX, PLOIDY

                ploidy_cmd = "--ploidy-file %s"%ploidy_file_bcftools

            cmd_bcftools_call = "%s call -m -f GQ,GP -v -O v --threads %i -o %s %s %s"%(bcftools, opt.threads, call_output_tmp, ploidy_cmd, mpileup_output); fun.run_cmd(cmd_bcftools_call)

            os.rename(call_output_tmp, call_output)
      
        #As there are no recommendations for bcftools, we decided to apply exclusively the filter for coverage. To apply harder filters please edit this command!
        
        # this generates a filtered, vcf, which only has the PASS ones.
        filtered_output_tmp = "%s.tmp"%filtered_output
        if fun.file_is_empty(filtered_output) or opt.replace is True:
            print("Filtering bcftools ... ")
            cmd_filter = "%s filter -m x -e 'INFO/DP <= %i' -O v --threads %i -o %s %s"%(bcftools, opt.min_coverage, opt.threads, filtered_output_tmp, call_output); fun.run_cmd(cmd_filter)
            os.rename(filtered_output_tmp, filtered_output)

        # keep
    filtered_vcf_results.append(filtered_output)

if "freebayes" in callers:
    fun.print_with_runtime("running freebayes")

    # create a folder that will contain the output of VCF
    outdir_freebayes = "%s/freebayes_ploidy%i_out"%(opt.outdir, opt.ploidy)

    # run freebayes in normal configuratiom, in parallel for each chromosome
    freebayes_filtered = fun.run_freebayes_parallel_regions(outdir_freebayes, opt.ref, sorted_bam, opt.ploidy, opt.min_coverage, replace=opt.replace, threads=opt.threads, pooled_sequencing=opt.pooled_sequencing, window_fb=opt.window_freebayes_bp, fb_use_best_n_alleles=opt.fb_use_best_n_alleles) 

    # keep
    filtered_vcf_results.append(freebayes_filtered)
    

# kill the process
if opt.stop_before_VariantIntegration is True:
    fun.print_with_runtime("stopping after variant calling")
    sys.exit(0)

############################

######## VARIANT INTEGRATION ###########

# get the merged vcf records (these are multiallelic)
fun.print_with_runtime("getting merged vcf without multialleles")
merged_vcf_all = fun.merge_several_vcfsSameSample_into_oneMultiSample_vcf(filtered_vcf_results, opt.ref, opt.outdir, opt.ploidy, replace=opt.replace, threads=opt.threads, repeats_table=repeats_table)

# get the variants in a tabular format
variantInfo_table = "%s/variant_calling_ploidy%i.tab"%(opt.outdir, opt.ploidy)
df_variants = fun.write_variantInfo_table(merged_vcf_all, variantInfo_table, replace=opt.replace)

# define the used programs as a list
all_programs = sorted(callers)

# generate a report of the variant calling
variantCallingStats_tablePrefix = "%s/variant_calling_stats_ploidy%i"%(opt.outdir, opt.ploidy)
fun.report_variant_calling_statistics(df_variants, variantCallingStats_tablePrefix, all_programs)


# add the CN state of each variant
if opt.outdir_callCNVs is not None: 
    fun.print_with_runtime("adding CN state")

    # load df with CNV
    df_CNV_coverage = fun.get_tab_as_df_or_empty_df("%s/final_CNVcalling.tab"%opt.outdir_callCNVs)

    # add the CN state
    fun.get_small_variant_calling_withCNstate(variantInfo_table, df_CNV_coverage, replace=opt.replace, range_overlap=50)

# keep vcfs that pass a certain number of programs
for minPASS_algs in [1, 2, 3]:

    simplified_vcf_PASSalgs = "%s/variants_atLeast%iPASS_ploidy%i.vcf"%(opt.outdir, minPASS_algs, opt.ploidy)
    simplified_vcf_PASSalgs_multialleleles = "%s/variants_atLeast%iPASS_ploidy%i.withMultiAlt.vcf"%(opt.outdir, minPASS_algs, opt.ploidy)
    simplified_vcf_PASSalgs_tmp = "%s.tmp"%simplified_vcf_PASSalgs

    if fun.file_is_empty(simplified_vcf_PASSalgs) or opt.replace is True:
        fun.print_with_runtime("getting vcf with vars called by >=%i programs"%minPASS_algs)

        # define the interesting variants
        df_PASS = df_variants[(df_variants["NPASS"]>=minPASS_algs) & (df_variants.mean_fractionReadsCov_PASS_algs>=min_AF) & (df_variants.mean_DP>=opt.min_coverage)]

        # add the FORMAT
        vcf_fields = ["#CHROM", "POS", "ID", "REF", "ALT", "QUAL", "FILTER", "INFO", "FORMAT", "SAMPLE"]

        # write empty vcf
        if len(df_PASS)==0: 
            open(simplified_vcf_PASSalgs, "w").write("\t".join(vcf_fields) + "\n")
            continue

        # rename the ID
        df_PASS = df_PASS.rename(columns={"#Uploaded_variation":"ID"})

        # set the FILTER ad the number of pass programs
        df_PASS["FILTER"] = df_PASS.NPASS.apply(str) + "xPASS"

        # set an empty INFO, unless the GT is unknown
        boolean_to_GTtag = {True:"unknown_GT", False:"known_GT"}
        df_PASS["INFO"] = (df_PASS.common_GT==".").apply(lambda x: boolean_to_GTtag[x])

        # set the format
        df_PASS["FORMAT"] = "GT:AF:DP:AD"

        # check that there are no NaNs in mean_fractionReadsCov_PASS_algs and mean_DP
        for field in ["mean_fractionReadsCov_PASS_algs", "mean_DP", "common_GT", "mean_AD"]: 
            if any(pd.isna(df_PASS[field])): 
                df_nan = df_PASS[pd.isna(df_PASS[field])]
                print(df_nan, df_nan.mean_fractionReadsCov_PASS_algs, df_nan.mean_DP, df_nan["ID"])
                raise ValueError("There are NaNs in %s"%field)

        # add the sample according to FORMAT
        df_PASS["SAMPLE"] = df_PASS.common_GT.apply(str) + ":" + df_PASS.mean_fractionReadsCov_PASS_algs.apply(lambda x: "%.4f"%x) + ":" + df_PASS.mean_DP.apply(lambda x: "%.4f"%x) + ":" + df_PASS.mean_AD

        # initialize header lines
        valid_header_starts = ["fileformat", "contig", "reference", "phasing"]
        header_lines = [l for l in fun.get_df_and_header_from_vcf(merged_vcf_all)[1] if any([l.startswith("##%s"%x) for x in valid_header_starts])]

        # add headers
        header_lines += ['##FILTER=<ID=1xPASS,Description="The variant PASSed the filters for 1 algorithm">',
                         '##FILTER=<ID=2xPASS,Description="The variant PASSed the filters for 2 algorithms">',
                         '##FILTER=<ID=3xPASS,Description="The variant PASSed the filters for 3 algorithms">',

                         '##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype. If there are discrepacncies in the GT between the algorithms where this var PASSed the filters GT is set to .">',
                         '##FORMAT=<ID=DP,Number=1,Type=Float,Description="Mean read depth of the locus from algorithms where this variant PASSed the filters">',
                         '##FORMAT=<ID=AF,Number=A,Type=Float,Description="Mean fraction of reads covering the ALT allele from algorithms where this variant PASSed the filters">',
                         '##FORMAT=<ID=AD,Number=R,Type=Integer,Description="The number of reads of each allele, as a mean of algorithms where this variant PASSed the filters">',

                         "##phasing=none",
                         '##source=%s'%("_".join(all_programs))]


        # write the split with split multialleles
        open(simplified_vcf_PASSalgs_tmp, "w").write("\n".join(header_lines) + "\n" + df_PASS[vcf_fields].to_csv(sep="\t", index=False, header=True))
        
        # write the split with joined multialleles
        if opt.ploidy==2 and opt.pooled_sequencing is False: fun.get_vcf_with_joined_multialleles_diploid(simplified_vcf_PASSalgs_tmp, simplified_vcf_PASSalgs_multialleleles, opt.ref, replace=opt.replace, threads=opt.threads)

        # rename the simplified
        os.rename(simplified_vcf_PASSalgs_tmp, simplified_vcf_PASSalgs)

########################################

#################################
#################################
#################################


##################################
########## CLEAN OUTPUT ##########
##################################

# clean
fun.delete_file_or_folder(reference_genome_dir)
fun.remove_smallVarsCNV_nonEssentialFiles(opt.outdir, opt.ploidy)

# wite final file
fun.generate_final_file_report_one_module(final_file, start_time, time.time())

# print the message
fun.print_with_runtime("perSVade %s finished correctly"%module_name)

##################################
##################################
##################################
