#!/usr/bin/env python

######################################
############# DEFINE ENV #############
######################################

# general module imports
import argparse, os
from argparse import RawTextHelpFormatter
import copy as cp
import pickle
import string
import shutil 
import random
import sys
from shutil import copyfile
import time
import pandas as pd
import multiprocessing as multiproc

# get the cwd were all the scripts are 
CWD = "/".join(__file__.split("/")[0:-1]); sys.path.insert(0, CWD)

# define the module name
module_name = __file__.split("/")[-1]

# define the EnvDir where the environment is defined
EnvDir = "/".join(sys.executable.split("/")[0:-2])

# import functions
import sv_functions as fun

# import perSVade-specific modules

######################################
######################################
######################################


#################################### 
############## ARGS ################
####################################

description = """
Analyzes how different SV calling parameters (provided with --SV_parameters) work on various SV simulations (provided with --optimize_parameters_outdirs). It generates some plots and also a set of parameters that work well across all.
"""

# mandatory
parser = argparse.ArgumentParser(description=description, formatter_class=RawTextHelpFormatter)
parser.add_argument("-o", "--outdir", dest="outdir", action="store", required=True, help="Output directory.")
parser.add_argument("--SV_parameters", dest="SV_parameters", action="store", required=True, help="Tab-sepparated file with 'parameterID' and 'parameters_json'")
parser.add_argument("--optimize_parameters_outdirs", dest="optimize_parameters_outdirs", action="store", required=True, help="Tab-sepparated file with 'sampleID' and 'optimize_parameters_outdir'.")
parser.add_argument("-r", "--ref", dest="ref", required=True, help="Reference genome. It has to end with .fasta.")
parser.add_argument("-mchr", "--mitochondrial_chromosome", dest="mitochondrial_chromosome", required=True, type=str, help="The name of the mitochondrial chromosome. If there is no mitochondria just put 'no_mitochondria'. If there is more than one mitochindrial scaffold, provide them as comma-sepparated IDs, like '--mitochondrial_chromosome chr_mito_1,chr_mito_2'.")
parser.add_argument("--repeats_file", dest="repeats_file", required=True, help="A file with the repeats of the reference genome, such as the file 'combined_repeats.tab' generated by perSVade infer_repeats. You may set '--repeats_file skip' if you don't want to consider repeats for SV filtering.")

# optionals
parser.add_argument("--replace", dest="replace", action="store_true", help="Re-run all the steps by deleting the output directory.")
parser.add_argument("--verbose", dest="verbose", action="store_true", default=False, help="Print a verbose log.")
parser.add_argument("--min_chromosome_len", dest="min_chromosome_len", default=100000, type=int, help="The minimum length to consider chromosomes from the provided fasta for calculating the window length (used in may steps of perSVade to parallelize across fractions of the genome).")
parser.add_argument("--Gb_per_thread", dest="Gb_per_thread", default=8, type=int, help="# Gb per thread, for parallel running.")

# resources
parser.add_argument("--fraction_available_mem", dest="fraction_available_mem", default=None, type=float, help="This pipeline calculates the available RAM for several steps, and it may not work well in some systems (i.e. HPC clusters). This parameter allows you to correct possible errors. If --fraction_available_mem is not provided (default behavior), this pipeline will calculate the available RAM by filling the memory, which may give errors. If you want to use all the available memory you should specify --fraction_available_mem 1.0. See the FAQ 'How does the --fraction_available_mem work?' from https://github.com/Gabaldonlab/perSVade/wiki/8.-FAQs for more info.")
parser.add_argument("-thr", "--threads", dest="threads", default=16, type=int, help="Number of threads, Default: 16")
parser.add_argument("--fractionRAM_to_dedicate", dest="fractionRAM_to_dedicate", type=float,  default=0.5, help="This is the fraction of the available memory that will be used by several java programs that require a heap size. By default we set this to 0.5 to not overload the system.")

opt = parser.parse_args()

####################################
####################################
####################################

#################################
########### MAIN CODE ###########
#################################

# remove outdir if replace, and set replace to False
if opt.replace is True: fun.delete_folder(opt.outdir)
opt.replace = False

# make the outdir
fun.make_folder(opt.outdir)

# exit if the final file exists
final_file = "%s/perSVade_finished_file.txt"%opt.outdir

if not fun.file_is_empty(final_file): 
    fun.print_with_runtime("WARNING: %s exists, suggesting that perSVade was already  run in this folder. Remove this file if you want this command to work. Exiting..."%final_file)
    sys.exit(0)

# define the start time
start_time = time.time()

# define the verbosity. If opt.verbose is False, none of the 'print' statements of sv_functions will have an effect
fun.printing_verbose_mode = opt.verbose

# define a file that will contain all the cmds ran
fun.log_file_all_cmds = "%s/all_cmds.txt"%opt.outdir
if fun.file_is_empty(fun.log_file_all_cmds): open(fun.log_file_all_cmds, "w").write("# These are all the cmds:\n")

# get sample name
sample_name = fun.get_sampleName_from_perSVade_outdir(opt.outdir)

####### SET RESOURCES ########

# define the fraction of RAM to dedicate
if opt.fractionRAM_to_dedicate>0.95: raise ValueError("You are using >95 pct of the systems RAM, which is dangerous")
fun.fractionRAM_to_dedicate = opt.fractionRAM_to_dedicate

# define the fraction of available mem
fun.fraction_available_mem = opt.fraction_available_mem
if opt.fraction_available_mem is None: fun.print_with_runtime("WARNING: You did not specify how much RAM should be used through --fraction_available_mem. perSVade will calculate this by filling the memory, which may be dangerous. If you want to use all the allocated memory you should specify --fraction_available_mem 1.0")

# print the available resources
real_available_threads = fun.get_available_threads(opt.outdir)
if opt.threads>real_available_threads:  fun.print_with_runtime("WARNING: There are %i available threads, and you required %i."%(real_available_threads, opt.threads))

available_Gb_RAM = fun.get_availableGbRAM(opt.outdir)
fun.print_with_runtime("Running perSVade %s into %s with %.3f Gb of RAM and %i cores"%(module_name, opt.outdir, available_Gb_RAM, opt.threads))

# prepare the reference genome
opt.ref, reference_genome_dir = fun.prepare_reference_genome_for_perSVade(opt.ref, opt.outdir, opt.mitochondrial_chromosome, None, opt.replace)
fun.window_l = fun.get_perSVade_window_l(opt.ref, opt.mitochondrial_chromosome, opt.min_chromosome_len)

# prepare the repeats file
fun.prepare_repeats_file_for_perSVade(opt.repeats_file, opt.ref)

# clean reference
fun.clean_reference_genome_windows_files(opt.ref)

# init a tmp folder
tmpdir = "%s/tmp"%opt.outdir; fun.make_folder(tmpdir)

##############################

##### GET DF ####

# load dfs
df_SV_parameters = pd.read_csv(opt.SV_parameters, sep="\t")[["parameterID", "parameters_json"]].sort_values(by=["parameterID"])
df_optimize_parameters_outdirs = pd.read_csv(opt.optimize_parameters_outdirs, sep="\t")[["sampleID", "optimize_parameters_outdir"]].sort_values(by=["sampleID"])

# get df of parameters
parms_dict = {}
for parameterID, parameters_json  in df_SV_parameters.values:    
    gridss_blacklisted_regions, gridss_maxcoverage, gridss_filters_dict, max_rel_coverage_to_consider_del, min_rel_coverage_to_consider_dup = fun.get_SVcalling_parameters(parameters_json)

    parms_dict[parameterID] = {"parameterID":parameterID, "gridss_blacklisted_regions":gridss_blacklisted_regions, "gridss_maxcoverage":gridss_maxcoverage, "max_rel_coverage_to_consider_del":max_rel_coverage_to_consider_del, "min_rel_coverage_to_consider_dup":min_rel_coverage_to_consider_dup}

    for k,v in gridss_filters_dict.items(): parms_dict[parameterID][k] = v

df_parms = pd.DataFrame(parms_dict).transpose().reset_index(drop=True)

# add str ID
parm_fields = sorted(set(df_parms.keys()).difference({"parameterID"}))
df_parms["parms_str"] = df_parms[parm_fields].apply(lambda r: "_".join(["%s=%s"%(f, r[f]) for f in parm_fields]), axis=1)

# checks
default_gridss_blacklisted_regions, default_gridss_maxcoverage = fun.get_SVcalling_parameters("default")[0:2]
if not all(df_parms.gridss_blacklisted_regions==default_gridss_blacklisted_regions): 
    raise ValueError("gridss_blacklisted_regions should be %s"%default_gridss_blacklisted_regions)

if not all(df_parms.gridss_maxcoverage==default_gridss_maxcoverage): 
    raise ValueError("gridss_maxcoverage should be %s"%default_gridss_maxcoverage)

if len(set(df_parms.parms_str))==1: raise ValueError("all parms are the same.")
if len(set(df_parms.parameterID))!=len(df_parms): raise ValueError("parameterID should be unique")

# add to df_SV_parameters
df_SV_parameters = df_SV_parameters.merge(df_parms[["parameterID", "parms_str"]], on="parameterID", validate="one_to_one", how="left")
df_SV_parameters = df_SV_parameters.sort_values(by=["parms_str", "parameterID"])

# get the unique ones
df_SV_parameters_unique = df_SV_parameters.drop_duplicates(subset=["parms_str"], keep="first").copy()
parms_str_to_rep_parmID = dict(df_SV_parameters_unique.set_index("parms_str").parameterID)
df_SV_parameters["rep_parameterID"] = df_SV_parameters.parms_str.apply(lambda x: parms_str_to_rep_parmID[x])
fun.print_if_verbose("There are %i/%i unique parameters"%(len(df_SV_parameters_unique), len(df_SV_parameters)))

# save
fun.save_df_as_tab(df_SV_parameters[["parameterID", "rep_parameterID", "parameters_json"]], "%s/SV_parameters.tab"%opt.outdir)

# add extra, unique parameters
if not "default" in df_SV_parameters.parameterID:
    df_extra_parms = pd.DataFrame({"parameterID":["default"], "parameters_json":["default"]})
    df_SV_parameters_unique = pd.concat([df_SV_parameters_unique, df_extra_parms])

# get the evaluation
df_evaluation_all_file = "%s/df_evaluation_unique_parameters.tab"%opt.outdir
if fun.file_is_empty(df_evaluation_all_file):

    # generate a train-test df with the effects of different parameters
    inputs_fn = []
    fun.print_if_verbose("preparing inputs")
    for Ip, rparm in df_SV_parameters_unique.iterrows():
        parameterID = rparm.parameterID
        parameters_json = rparm.parameters_json

        # get parameters
        gridss_blacklisted_regions, gridss_maxcoverage, gridss_filters_dict, max_rel_coverage_to_consider_del, min_rel_coverage_to_consider_dup = fun.get_SVcalling_parameters(parameters_json)

        # iterate through each sample
        for sampleID, optimize_parameters_outdir in df_optimize_parameters_outdirs.values:

            # define testing elements
            simulation_files_dir = "%s/simulation_files"%optimize_parameters_outdir
            genomeIDs = sorted({f.split("=")[1].split("-")[0] for f in os.listdir(simulation_files_dir)})
            ploidies = sorted({f.split("=")[2].split(".")[0] for f in os.listdir(simulation_files_dir)})

            # test for each simulation and ploidy
            for genomeID in genomeIDs:
                for ploidy in ploidies:

                    # get the input files
                    fileprefix = "%s/genome=%s-ploidy=%s"%(simulation_files_dir, genomeID, ploidy)
                    dict_sequencing_features = fun.load_object("%s.sequencing_features.py"%fileprefix)
                    SV_dict = {f.split(".")[-2] : "%s/%s"%(simulation_files_dir, f) for f in os.listdir(simulation_files_dir) if f.startswith(fun.get_file(fileprefix)) and ".simulated_SVs." in f}

                    # get files under common filedir
                    test_dir = "%s/evaluating_%s_on_%s_genome=%s-ploidy=%s"%(tmpdir, parameterID, sampleID, genomeID, ploidy); fun.make_folder(test_dir)
                    
                    # get the cmds
                    inputs_fn.append(("%s.aligned_reads.bam.sorted"%fileprefix, "%s.gridss_output.vcf"%fileprefix, SV_dict, dict_sequencing_features, opt.ref, test_dir, gridss_blacklisted_regions, gridss_maxcoverage, gridss_filters_dict, max_rel_coverage_to_consider_del, min_rel_coverage_to_consider_dup, parameterID, sampleID, genomeID, ploidy))

    
    # run in parallel
    threads_run = min([int(available_Gb_RAM/opt.Gb_per_thread), opt.threads])
    fun.print_if_verbose("Running in parallel on %i threads"%threads_run)

    with multiproc.Pool(threads_run) as pool:
        df_evaluation_all = pd.concat(pool.starmap(fun.get_df_evaluation_analyze_SV_parameters, inputs_fn, chunksize=1)).reset_index(drop=True)
        pool.close()
        pool.terminate()

    # save
    fun.save_df_as_tab(df_evaluation_all, df_evaluation_all_file)

################

###### EVALUATIONS #####

# load
df_evaluation_all = fun.get_tab_as_df_or_empty_df(df_evaluation_all_file)

# plot heatmap 
fun.print_if_verbose("Accuracy heatmaps")
for accuracy_field in ["Fvalue", "precision", "recall"]:
    fun.plot_heatmap_analyze_SV_parameters(df_evaluation_all, "%s/heatmap_parameters_%s.pdf"%(opt.outdir, accuracy_field), accuracy_field)

# sort parameters from less to more conservative

# keep df
df_parms_initial = df_parms.copy()

# init with the general filters of perSVade
sorting_fields = ["max_rel_coverage_to_consider_del", "min_rel_coverage_to_consider_dup", "gridss_blacklisted_regions", "gridss_maxcoverage"]
ascending_list = [False, True, True, True]

# add the gridss filterings
gridss_sorting_fields = ["min_Nfragments", "min_af", "min_af_EitherSmallOrLargeEvent", "wrong_FILTERtags", "min_QUAL", "filter_noReadPairs", "filter_noSplitReads", "min_size", "filter_overlappingRepeats", "maximum_strand_bias", "maximum_microhomology", "maximum_lenght_inexactHomology", "dif_between_insert_and_del", "range_filt_DEL_breakpoints", "max_to_be_considered_small_event", "min_length_inversions", "filter_polyGC", "wrong_INFOtags"]
sorting_fields += gridss_sorting_fields

# rename some fields to be numeric, as defined in fun.g_filterName_to_filterValue_to_Number
non_numeric_fields = {"wrong_FILTERtags", "filter_polyGC", "wrong_INFOtags", "filter_noReadPairs", "filter_noSplitReads", "filter_overlappingRepeats", "range_filt_DEL_breakpoints"}
for f in non_numeric_fields: 
    df_parms[f] = df_parms[f].apply(lambda x: fun.g_filterName_to_filterValue_to_Number[f][x])
    if any(pd.isna(df_parms[f])): raise ValueError("nans in %s"%f)

# add the ascendings
for f in gridss_sorting_fields:
    if f in non_numeric_fields: ascending_list.append(True)
    else:

        template_sorted_vals = fun.g_filterName_to_filtersList[f]
        if template_sorted_vals==sorted(template_sorted_vals): ascending_list.append(True)
        elif template_sorted_vals==list(reversed(sorted(template_sorted_vals))): ascending_list.append(False)
        else: raise ValueError("invalid %s"%template_sorted_vals)

# convert to floats
for f in gridss_sorting_fields:
    df_parms[f] = df_parms[f].apply(float)  

# add the parameter ID
sorting_fields.append("parameterID")
ascending_list.append(True)

print(sorting_fields, set(df_parms_initial["min_Nfragments"]))


# check that you are considering all fields
missing_fields = set(df_parms.keys()).difference(set(sorting_fields).union({"parms_str"}))
if len(missing_fields)>0: raise ValueError("missing_fields: %s"%missing_fields)

# get the sorted filters filters
df_parms_sorted = df_parms[sorting_fields].sort_values(by=sorting_fields, ascending=ascending_list)
sorted_parameters = list(df_parms_sorted.parameterID)

# print the sorted parameters
changing_fields = [f for f in sorting_fields if len(set(df_parms_initial[f]))>=2 and f!="parameterID"]
print("These are the sorted parameters from least to most conservative (only showing changing fields)\n:", 
    df_parms_initial.set_index("parameterID").loc[sorted_parameters][changing_fields])

print("These are the fixed parameters:\n", "\n".join(["%s=%s"%(f, df_parms_initial[f].iloc[0]) for f in sorting_fields if len(set(df_parms_initial[f]))==1]))

# write the least and most conservative parameters
for parm_name, parm_idx in [("less_conservative", 0), ("more_conservative", -1)]:
    parmID = sorted_parameters[parm_idx]
    origin_json = df_SV_parameters[df_SV_parameters.parameterID==parmID].iloc[0].parameters_json
    fun.rsync_file(origin_json, "%s/%s_parameters_%s.json"%(opt.outdir, parm_name, parmID))

########################




##############

#################################
#################################
#################################


##################################
########## CLEAN OUTPUT ##########
##################################

# clean
fun.delete_folder(tmpdir)
fun.delete_folder(reference_genome_dir)

# wite final file
fun.generate_final_file_report_one_module(final_file, start_time, time.time())

# print the message
fun.print_with_runtime("perSVade %s finished correctly"%module_name)

##################################
##################################
##################################
